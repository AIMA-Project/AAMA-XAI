{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "import shap\n",
    "\n",
    "# Data operations, import, parsing, preprocessing\n",
    "# open data extracted from Cuckoo reports\n",
    "with open('extractedInfo.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "completeCallList = data['completeCallList']\n",
    "allFilesList = data['allFilesList']\n",
    "finishedRows = data['finishedRows']\n",
    "malwareList = data['malwareList']\n",
    "\n",
    "# create dataframe \n",
    "df = pd.DataFrame(columns=completeCallList)\n",
    "\n",
    "# put statistics from API calls in working dataframe\n",
    "count = 0\n",
    "for malwareSampleData in finishedRows:\n",
    "    df.loc[count] = malwareSampleData\n",
    "    count+=1\n",
    "\n",
    "# add truth label to dataset, take off as needed\n",
    "df['Malware'] = malwareList\n",
    "\n",
    "# drop truth label from training set, define training and testing sets\n",
    "X = df.drop('Malware', axis=1)  \n",
    "y = df['Malware']\n",
    "\n",
    "# train/test split. 80/20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 2020, stratify=y)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "# API calls (features) obtained for graphics\n",
    "features = X_test.columns\n",
    "\n",
    "print(\"This dataset consists of frequency feature behavior collected from 105\",\n",
    "      \"malicious and benign samples collected through cuckoo sandbox on a\", \n",
    "      \"Windows 7 machine. 216 were observed. The list of observed system\", \n",
    "      \"calls is below:\\n\\n\")\n",
    "for feature in features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Random Forest Classifier RFC\n",
    "rfc_clf = RandomForestClassifier()\n",
    "\n",
    "# optimize hyperparameters using gridsearch\n",
    "parameters = {'n_estimators':[10, 50, 100, 150, 200, 250, 300, 325, 350, 375, 400, 425, 450, 475, 500, 1000],\n",
    "              'criterion':('gini', 'entropy')}\n",
    "rfc_grid = GridSearchCV(rfc_clf, parameters, n_jobs=-1)\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "\n",
    "# take best model from gridsearch\n",
    "rfc_model = rfc_grid.best_estimator_\n",
    "\n",
    "# make some predictions\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "# RFC metrics\n",
    "rfc_acc = accuracy_score(y_test, rfc_pred)\n",
    "rfc_precision = precision_score(y_test, rfc_pred)\n",
    "rfc_recall = recall_score(y_test, rfc_pred)\n",
    "rfc_f1 = f1_score(y_test, rfc_pred)\n",
    "\n",
    "# print RFC scores\n",
    "print('\\nRandom Forest Classifier')\n",
    "print('Best Hyperparameters:', rfc_grid.best_params_)\n",
    "print('Accuracy: %.3f' % rfc_acc)\n",
    "print('Precision: %.3f' % rfc_precision)\n",
    "print('Recall: %.3f' % rfc_recall)\n",
    "print('F1 score: %.3f' % rfc_f1)\n",
    "\n",
    "# show RFC confusion matrix\n",
    "rfc_cm = confusion_matrix(y_test, rfc_pred, labels=rfc_model.classes_)\n",
    "rfc_disp = ConfusionMatrixDisplay(confusion_matrix=rfc_cm, display_labels=rfc_model.classes_)\n",
    "rfc_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC explanation using SHapely Additive exPlanations (SHAP)\n",
    "\n",
    "# set up js for rendering SHAP graphics\n",
    "shap.initjs()\n",
    "\n",
    "# shap for tree explainer on RFC, summary plot\n",
    "# note: max_display limits number of features displayed in order of importance.\n",
    "#       20 is default, but currently set to 216 to display all features.Right \n",
    "#       click an output image and select 'open image in new tab' to \n",
    "#       inspect more easily.\n",
    "print(\"RFC Summary Plot\")\n",
    "rfc_explainer = shap.TreeExplainer(rfc_model)\n",
    "rfc_shap_values = rfc_explainer.shap_values(X_test)\n",
    "shap.summary_plot(rfc_shap_values, features, class_names=[\"Benign\", \"Malicious\"], max_display=216, plot_type=\"bar\", plot_size=\"auto\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC Waterfall plot for 1 prediction\n",
    "print(\"\\n\\n\\nRFC Waterfall Plot\")\n",
    "rfc_shap_vals = rfc_explainer(X_test)\n",
    "rfc_exp = shap.Explanation(rfc_shap_vals.values[:,:,1],\n",
    "                      rfc_shap_vals.base_values[:,1],\n",
    "                      data=X_test.values,\n",
    "                      feature_names=features)\n",
    "idx=1\n",
    "# max_display default is 10 for good visibility. Currently set to 216\n",
    "# to make apparent which of all features push the prediction towards\n",
    "# malicious or benign, no matter how small that contribution is.\n",
    "shap.plots.waterfall(rfc_exp[idx], max_display=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a very simple Feed-Forward Neural Network FFNN\n",
    "ffnn_model = keras.Sequential([\n",
    "    layers.Dense(216, activation='relu', input_shape=(216,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "ffnn_model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                   metrics=['accuracy', keras.metrics.Precision(),\n",
    "                           keras.metrics.Recall()])\n",
    "ffnn_model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "\n",
    "# make some predictions, gather FFNN metrics\n",
    "ffnn_loss, ffnn_acc, ffnn_precision, ffnn_recall = ffnn_model.evaluate(X_test, y_test)\n",
    "ffnn_pred = ffnn_model.predict(X_test)\n",
    "ffnn_pred_binary = [1 if pred >= 0.5 else 0 for pred in ffnn_pred]\n",
    "\n",
    "# calc F1, avoid div by zero\n",
    "try:\n",
    "    ffnn_f1 = 2 * ((ffnn_precision * ffnn_recall) / (ffnn_precision + ffnn_recall))\n",
    "except:\n",
    "    ffnn_f1 = 0.0\n",
    "    print(\"F1 calc error\")\n",
    "\n",
    "# print FFNN scores\n",
    "print('\\nFeed Forward Neural Network')\n",
    "print('Loss:', ffnn_loss)\n",
    "print('Accuracy: %.3f' % ffnn_acc)\n",
    "print('Precision: %.3f' % ffnn_precision)\n",
    "print('Recall: %.3f' % ffnn_recall)\n",
    "print('F1 score: %.3f' % ffnn_f1)\n",
    "\n",
    "# show FFNN confusion matrix\n",
    "ffnn_cm = confusion_matrix(y_test, ffnn_pred_binary)\n",
    "ffnn_disp = ConfusionMatrixDisplay(confusion_matrix=ffnn_cm)\n",
    "ffnn_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN explanation using SHapely Additive exPlanations (SHAP)\n",
    "\n",
    "# shap for kernel explainer on FFNN, summary plot\n",
    "# note: max_display limits number of features displayed in order of importance.\n",
    "#       20 is default, but currently set to 216 to display all features. Right \n",
    "#       click an output image and select 'open image in new tab' to \n",
    "#       inspect more easily.\n",
    "ffnn_explainer = shap.KernelExplainer(ffnn_model, X_test)\n",
    "ffnn_shap_values = ffnn_explainer.shap_values(X_test)\n",
    "shap.summary_plot(ffnn_shap_values, features, class_names=[\"Benign\", \"Malicious\"], max_display=216, plot_type=\"bar\", plot_size=\"auto\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN Waterfall plot for 1 prediction\n",
    "# max_display default is 10 for good visibility. Currently set to 216\n",
    "# to make apparent which of all features push the prediction towards\n",
    "# malicious or benign, no matter how small that contribution is.\n",
    "shap.plots._waterfall.waterfall_legacy(ffnn_explainer.expected_value[0],\n",
    "                                       ffnn_shap_values[0][0],\n",
    "                                       feature_names=features,\n",
    "                                       max_display=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Multi-layer perceptron MLP\n",
    "mlp_clf = MLPClassifier()\n",
    "\n",
    "# optimize hyperparameters using gridsearch\n",
    "parameters = {'hidden_layer_sizes':[100, 150, 200], \n",
    "              'activation':('logistic', 'relu'),\n",
    "              'solver':('lbfgs', 'adam'),\n",
    "              'learning_rate':('constant', 'invscaling'),\n",
    "              'max_iter':[200, 500, 1000]}\n",
    "mlp_grid = GridSearchCV(mlp_clf, parameters, n_jobs=-1)\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "\n",
    "# take the best model from gridsearch\n",
    "mlp_model = mlp_grid.best_estimator_\n",
    "\n",
    "# make some predictions\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# MLP metrics\n",
    "mlp_acc = accuracy_score(y_test, mlp_pred)\n",
    "mlp_precision = precision_score(y_test, mlp_pred)\n",
    "mlp_recall = recall_score(y_test, mlp_pred)\n",
    "mlp_f1 = f1_score(y_test, mlp_pred)\n",
    "\n",
    "# print MLP scores\n",
    "print('\\nMulti-layer Perceptron')\n",
    "print('Best Hyperparameters:', mlp_grid.best_params_)\n",
    "print('Accuracy: %.3f' % mlp_acc)\n",
    "print('Precision: %.3f' % mlp_precision)\n",
    "print('Recall: %.3f' % mlp_recall)\n",
    "print('F1 score: %.3f' % mlp_f1)\n",
    "\n",
    "# show MLP confusion matrix\n",
    "mlp_cm = confusion_matrix(y_test, mlp_pred, labels=mlp_model.classes_)\n",
    "mlp_disp = ConfusionMatrixDisplay(confusion_matrix=mlp_cm, display_labels=mlp_model.classes_)\n",
    "mlp_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP explanation using SHapely Additive exPlanations (SHAP)\n",
    "\n",
    "# shap for kernel explainer on MLP, summary plot\n",
    "# note: max_display limits number of features displayed in order of importance.\n",
    "#       20 is default, but currently set to 216 to display all features. Right \n",
    "#       click an output image and select 'open image in new tab' to \n",
    "#       inspect more easily.\n",
    "mlp_explainer = shap.KernelExplainer(mlp_model.predict, X_test)\n",
    "mlp_shap_values = mlp_explainer.shap_values(X_test)\n",
    "shap.summary_plot(mlp_shap_values, features, class_names=[\"Benign\", \"Malicious\"], max_display=216, plot_type=\"bar\", plot_size=\"auto\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Waterfall plot for 1 prediction\n",
    "# max_display default is 10 for good visibility. Currently set to 216\n",
    "# to make apparent which of all features push the prediction towards\n",
    "# malicious or benign, no matter how small that contribution is.\n",
    "shap.plots._waterfall.waterfall_legacy(mlp_explainer.expected_value,\n",
    "                                       mlp_shap_values[0],\n",
    "                                       feature_names=features,\n",
    "                                       max_display=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define support vector machine SVM\n",
    "svm_clf = svm.SVC()\n",
    "\n",
    "# optimize hyperparameters using gridsearch\n",
    "parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'C':[1, 5, 10, 25, 50, 75, 100, 500]}\n",
    "svm_grid = GridSearchCV(svm_clf, parameters, n_jobs=-1)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# take the best model from gridsearch\n",
    "svm_model = svm_grid.best_estimator_\n",
    "\n",
    "# make some predictions\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# svm metrics\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "svm_precision = precision_score(y_test, svm_pred)\n",
    "svm_recall = recall_score(y_test, svm_pred)\n",
    "svm_f1 = f1_score(y_test, svm_pred)\n",
    "\n",
    "# print SVM scores\n",
    "print('\\nSupport Vector Machine')\n",
    "print('Best Hyperparameters:', svm_grid.best_params_)\n",
    "print('Accuracy: %.3f' % svm_acc)\n",
    "print('Precision: %.3f' % svm_precision)\n",
    "print('Recall: %.3f' % svm_recall)\n",
    "print('F1 score: %.3f' % svm_f1)\n",
    "\n",
    "# show SVM confusion matrix\n",
    "svm_cm = confusion_matrix(y_test, svm_pred, labels=svm_model.classes_)\n",
    "svm_disp = ConfusionMatrixDisplay(confusion_matrix=svm_cm, display_labels=svm_model.classes_)\n",
    "svm_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM explanation using SHapely Additive exPlanations (SHAP)\n",
    "\n",
    "# shap for kernel explainer on SVM, summary plot\n",
    "# note: max_display limits number of features displayed in order of importance.\n",
    "#       20 is default, but currently set to 216 to display all features. Right \n",
    "#       click an output image and select 'open image in new tab' to \n",
    "#       inspect more easily.\n",
    "\n",
    "svm_explainer = shap.KernelExplainer(svm_model.predict, X_test)\n",
    "svm_shap_values = svm_explainer.shap_values(X_test)\n",
    "shap.summary_plot(svm_shap_values, features, class_names=[\"Benign\", \"Malicious\"], max_display=216, plot_type=\"bar\", plot_size=\"auto\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Waterfall plot for 1 prediction\n",
    "# max_display default is 10 for good visibility. Currently set to 216\n",
    "# to make apparent which of all features push the prediction towards\n",
    "# malicious or benign, no matter how small that contribution is.\n",
    "shap.plots._waterfall.waterfall_legacy(svm_explainer.expected_value,\n",
    "                                       svm_shap_values[0],\n",
    "                                       feature_names=features,\n",
    "                                       max_display=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
